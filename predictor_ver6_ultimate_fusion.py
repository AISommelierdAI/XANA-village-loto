import csv
import json
import random
import math
from collections import Counter, defaultdict
from datetime import datetime
import os

class TotoVer6UltimateFusion:
    """ToToã€‡ãã‚“ Ver.6 Ultimate Fusion - å…¨æ©Ÿèƒ½çµ±åˆæ¬¡ä¸–ä»£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, csv_file='totomaru.csv'):
        self.csv_file = csv_file
        self.ai_weights = self.initialize_ai_weights()
        self.learning_history = self.load_learning_history()
        self.ensure_results_dir()
    
    def ensure_results_dir(self):
        """çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿"""
        if not os.path.exists('results'):
            os.makedirs('results')
    
    def initialize_ai_weights(self):
        """AIé‡ã¿ã®åˆæœŸåŒ–ï¼ˆå…¨æ©Ÿèƒ½çµ±åˆç‰ˆï¼‰"""
        return {
            'range_balance': 0.15,
            'consecutive_pattern': 0.12,
            'frequency_analysis': 0.12,
            'temporal_trend': 0.10,
            'statistical_optimization': 0.12,
            'learning_adaptation': 0.08,
            'monte_carlo': 0.10,
            'markov_chain': 0.08,
            'time_series': 0.08,
            'fourier_analysis': 0.05
        }
    
    def load_learning_history(self):
        """å­¦ç¿’å±¥æ­´ã®èª­ã¿è¾¼ã¿"""
        try:
            with open('learning_history.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                'success_patterns': [],
                'failure_patterns': [],
                'accuracy_trends': [],
                'weight_adjustments': []
            }
    
    def save_learning_history(self):
        """å­¦ç¿’å±¥æ­´ã®ä¿å­˜"""
        with open('learning_history.json', 'w', encoding='utf-8') as f:
            json.dump(self.learning_history, f, ensure_ascii=False, indent=2)
    
    def load_data(self):
        """CSVãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            data = []
            with open(self.csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    try:
                        # BOMã‚’é™¤å»ã—ã¦ã‚­ãƒ¼ã‚’æ­£è¦åŒ–
                        normalized_row = {}
                        for key, value in row.items():
                            normalized_key = key.replace('\ufeff', '')
                            normalized_row[normalized_key] = value
                        
                        if normalized_row['DrawDate'] and normalized_row['Number1'] and normalized_row['Number1'].strip():
                            numbers = [
                                int(normalized_row['Number1']), int(normalized_row['Number2']), int(normalized_row['Number3']),
                                int(normalized_row['Number4']), int(normalized_row['Number5']), int(normalized_row['Number6'])
                            ]
                            data.append({
                                'date': normalized_row['DrawDate'],
                                'numbers': numbers,
                                'additional': int(normalized_row['Additional']) if normalized_row['Additional'] and normalized_row['Additional'].strip() else 0
                            })
                    except (ValueError, KeyError) as e:
                        continue
            return data
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def analyze_all_functions(self, data):
        """å…¨æ©Ÿèƒ½çµ±åˆåˆ†æ"""
        analysis = {}
        
        # åŸºæœ¬åˆ†æ
        analysis['range_analysis'] = self.analyze_range_distribution(data)
        analysis['consecutive_analysis'] = self.analyze_consecutive_patterns(data)
        analysis['frequency_analysis'] = self.analyze_frequency_patterns(data)
        analysis['temporal_analysis'] = self.analyze_temporal_patterns(data)
        
        # é«˜åº¦ãªæ•°å­¦çš„åˆ†æ
        analysis['statistical_tests'] = self.perform_statistical_tests(data)
        analysis['fourier_analysis'] = self.analyze_fourier_patterns(data)
        analysis['bayesian_analysis'] = self.analyze_bayesian_probabilities(data)
        analysis['theoretical_analysis'] = self.analyze_theoretical_probability(data)
        
        # æ–°åˆ†ææ‰‹æ³•
        analysis['monte_carlo_simulation'] = self.perform_monte_carlo_simulation(data)
        analysis['markov_chain_analysis'] = self.analyze_markov_chains(data)
        analysis['enhanced_time_series'] = self.enhanced_time_series_analysis(data)
        
        # å­¦ç¿’å±¥æ­´åˆ†æ
        analysis['learning_analysis'] = self.analyze_learning_patterns()
        
        return analysis
    
    def analyze_range_distribution(self, data):
        """ç¯„å›²åˆ†å¸ƒåˆ†æ"""
        ranges = {'low': [], 'mid': [], 'high': []}
        for draw in data[-30:]:
            for num in draw['numbers']:
                if 1 <= num <= 16:
                    ranges['low'].append(num)
                elif 17 <= num <= 32:
                    ranges['mid'].append(num)
                else:
                    ranges['high'].append(num)
        
        return {
            'low_freq': Counter(ranges['low']),
            'mid_freq': Counter(ranges['mid']),
            'high_freq': Counter(ranges['high']),
            'range_balance': {
                'low_ratio': len(ranges['low']) / (len(ranges['low']) + len(ranges['mid']) + len(ranges['high'])),
                'mid_ratio': len(ranges['mid']) / (len(ranges['low']) + len(ranges['mid']) + len(ranges['high'])),
                'high_ratio': len(ranges['high']) / (len(ranges['low']) + len(ranges['mid']) + len(ranges['high']))
            }
        }
    
    def analyze_consecutive_patterns(self, data):
        """é€£ç¶šæ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        consecutive_stats = {'pairs': Counter(), 'triples': Counter(), 'frequency': 0}
        
        for draw in data[-30:]:
            sorted_nums = sorted(draw['numbers'])
            consecutive_count = 0
            
            for i in range(len(sorted_nums) - 1):
                if sorted_nums[i+1] - sorted_nums[i] == 1:
                    consecutive_count += 1
                    pair = (sorted_nums[i], sorted_nums[i+1])
                    consecutive_stats['pairs'][pair] += 1
            
            if consecutive_count >= 2:
                consecutive_stats['frequency'] += 1
        
        consecutive_stats['frequency'] /= len(data[-30:])
        return consecutive_stats
    
    def analyze_frequency_patterns(self, data):
        """é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        all_numbers = []
        for draw in data[-50:]:
            all_numbers.extend(draw['numbers'])
        
        freq_counter = Counter(all_numbers)
        most_frequent = freq_counter.most_common(15)
        least_frequent = freq_counter.most_common()[:-16:-1]
        
        return {
            'most_frequent': most_frequent,
            'least_frequent': least_frequent,
            'frequency_distribution': dict(freq_counter)
        }
    
    def analyze_temporal_patterns(self, data):
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        temporal_cycles = defaultdict(list)
        
        for draw in data[-30:]:
            try:
                date_obj = datetime.strptime(draw['date'], '%Y-%m-%d')
                weekday = date_obj.weekday()
                temporal_cycles[weekday].extend(draw['numbers'])
            except:
                continue
        
        return dict(temporal_cycles)
    
    def perform_statistical_tests(self, data):
        """çµ±è¨ˆçš„æ¤œå®š"""
        observed_freq = Counter()
        for draw in data[-30:]:
            for num in draw['numbers']:
                observed_freq[num] += 1
        
        expected_freq = (6 * 30) / 49
        chi_square = sum(((observed_freq.get(num, 0) - expected_freq) ** 2) / expected_freq for num in range(1, 50))
        
        return {
            'chi_square_test': {
                'chi_square_statistic': chi_square,
                'degrees_of_freedom': 48,
                'p_value_estimate': self.estimate_p_value(chi_square, 48)
            }
        }
    
    def analyze_fourier_patterns(self, data):
        """ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›åˆ†æ"""
        try:
            import numpy as np
            
            time_series = {}
            for num in range(1, 50):
                time_series[num] = []
                for draw in data[-50:]:
                    time_series[num].append(1 if num in draw['numbers'] else 0)
            
            fourier_results = {}
            for num, series in time_series.items():
                if len(series) > 1:
                    fft = np.fft.fft(series)
                    power_spectrum = np.abs(fft) ** 2
                    dominant_freq = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1
                    fourier_results[num] = {
                        'dominant_frequency': dominant_freq,
                        'power': np.max(power_spectrum),
                        'periodicity_score': np.sum(power_spectrum[1:]) / len(power_spectrum)
                    }
            
            return fourier_results
        except ImportError:
            return {}
    
    def analyze_bayesian_probabilities(self, data):
        """ãƒ™ã‚¤ã‚ºçµ±è¨ˆåˆ†æ"""
        bayesian_results = {}
        prior_prob = 1/49
        
        for num in range(1, 50):
            appearances = sum(1 for draw in data[-20:] if num in draw['numbers'])
            total_draws = len(data[-20:])
            
            if total_draws > 0:
                likelihood = appearances / total_draws
                posterior_prob = (likelihood * prior_prob) / (likelihood * prior_prob + (1-likelihood) * (1-prior_prob))
                bayesian_results[num] = {
                    'posterior_probability': posterior_prob,
                    'appearances': appearances
                }
        
        return bayesian_results
    
    def analyze_theoretical_probability(self, data):
        """ç†è«–çš„ç¢ºç‡åˆ†æ"""
        total_draws = len(data)
        expected_freq = (6 * total_draws) / 49
        
        actual_freq = Counter()
        for draw in data:
            for num in draw['numbers']:
                actual_freq[num] += 1
        
        deviation_analysis = {}
        for num in range(1, 50):
            actual = actual_freq.get(num, 0)
            deviation = (actual - expected_freq) / expected_freq
            deviation_analysis[num] = {
                'actual': actual,
                'expected': expected_freq,
                'deviation': deviation
            }
        
        return {'deviation_analysis': deviation_analysis}
    
    def perform_monte_carlo_simulation(self, data):
        """ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        # æœ€è¿‘ã®å‡ºç¾é »åº¦ã«åŸºã¥ãç¢ºç‡åˆ†å¸ƒ
        recent_freq = Counter()
        for draw in data[-20:]:
            for num in draw['numbers']:
                recent_freq[num] += 1
        
        # ç¢ºç‡åˆ†å¸ƒã®æ­£è¦åŒ–
        total_appearances = sum(recent_freq.values())
        probability_distribution = {}
        for num in range(1, 50):
            probability_distribution[num] = recent_freq.get(num, 0) / total_appearances if total_appearances > 0 else 1/49
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        simulations = []
        for _ in range(1000):
            sim_numbers = []
            for _ in range(6):
                weights = [probability_distribution[i] for i in range(1, 50)]
                chosen = random.choices(range(1, 50), weights=weights)[0]
                if chosen not in sim_numbers:
                    sim_numbers.append(chosen)
            if len(sim_numbers) == 6:
                simulations.append(tuple(sorted(sim_numbers)))
        
        sim_counter = Counter(simulations)
        top_patterns = sim_counter.most_common(10)
        
        return {
            'probability_distribution': probability_distribution,
            'top_patterns': top_patterns
        }
    
    def analyze_markov_chains(self, data):
        """ãƒãƒ«ã‚³ãƒ•é€£é–åˆ†æ"""
        # æ•°å­—ã®é·ç§»ç¢ºç‡ã‚’è¨ˆç®—
        transitions = defaultdict(Counter)
        number_sequences = []
        
        for draw in data[-30:]:
            sorted_nums = sorted(draw['numbers'])
            number_sequences.append(sorted_nums)
            
            for i in range(len(sorted_nums) - 1):
                current = sorted_nums[i]
                next_num = sorted_nums[i + 1]
                transitions[current][next_num] += 1
        
        # é·ç§»ç¢ºç‡ã®è¨ˆç®—
        transition_probabilities = {}
        for current, next_counts in transitions.items():
            total = sum(next_counts.values())
            transition_probabilities[current] = {next_num: count/total for next_num, count in next_counts.items()}
        
        # å®šå¸¸çŠ¶æ…‹ç¢ºç‡ã®ç°¡æ˜“è¨ˆç®—
        steady_state = {}
        for num in range(1, 50):
            steady_state[num] = sum(1 for seq in number_sequences if num in seq) / len(number_sequences)
        
        return {
            'transition_probabilities': dict(transition_probabilities),
            'steady_state_probabilities': steady_state
        }
    
    def enhanced_time_series_analysis(self, data):
        """å¼·åŒ–æ™‚ç³»åˆ—åˆ†æ"""
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trend_analysis = {}
        for num in range(1, 50):
            appearances = []
            for i, draw in enumerate(data[-20:]):
                appearances.append(1 if num in draw['numbers'] else 0)
            
            if sum(appearances) > 0:
                # å˜ç´”ãªãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
                recent_trend = sum(appearances[-5:]) - sum(appearances[:5])
                trend_analysis[num] = {
                    'trend': recent_trend,
                    'recent_frequency': sum(appearances[-5:]) / 5,
                    'overall_frequency': sum(appearances) / len(appearances)
                }
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        momentum_indicators = {}
        for num in range(1, 50):
            recent_appearances = sum(1 for draw in data[-5:] if num in draw['numbers'])
            previous_appearances = sum(1 for draw in data[-10:-5] if num in draw['numbers'])
            
            momentum = recent_appearances - previous_appearances
            momentum_indicators[num] = {
                'momentum_strength': abs(momentum),
                'momentum_direction': 'positive' if momentum > 0 else 'negative' if momentum < 0 else 'neutral'
            }
        
        return {
            'trend_analysis': trend_analysis,
            'momentum_indicators': momentum_indicators
        }
    
    def analyze_learning_patterns(self):
        """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if not self.learning_history['success_patterns']:
            return {}
        
        # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å­¦ç¿’
        successful_numbers = []
        for success in self.learning_history['success_patterns'][-5:]:
            successful_numbers.extend(success['numbers'])
        
        success_freq = Counter(successful_numbers)
        
        return {
            'successful_numbers': dict(success_freq),
            'success_patterns': self.learning_history['success_patterns'][-3:]
        }
    
    def estimate_p_value(self, chi_square, df):
        """på€¤æ¨å®š"""
        if chi_square < df:
            return 0.5
        elif chi_square < df * 1.5:
            return 0.1
        elif chi_square < df * 2:
            return 0.05
        else:
            return 0.01
    
    def calculate_unified_confidence(self, pattern, all_analysis):
        """çµ±åˆä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 50.0  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # ç¯„å›²ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡
        range_analysis = all_analysis['range_analysis']
        low_count = len([n for n in pattern if 1 <= n <= 16])
        mid_count = len([n for n in pattern if 17 <= n <= 32])
        high_count = len([n for n in pattern if 33 <= n <= 49])
        
        balance_score = 1 - abs(low_count - mid_count) / 6 - abs(mid_count - high_count) / 6
        confidence += balance_score * 10
        
        # é »åº¦åˆ†æè©•ä¾¡
        freq_analysis = all_analysis['frequency_analysis']
        freq_score = 0
        for num in pattern:
            if num in freq_analysis['frequency_distribution']:
                freq_score += freq_analysis['frequency_distribution'][num]
        confidence += (freq_score / len(pattern)) * 5
        
        # ãƒ™ã‚¤ã‚ºç¢ºç‡è©•ä¾¡
        bayesian_analysis = all_analysis['bayesian_analysis']
        bayesian_score = 0
        for num in pattern:
            if num in bayesian_analysis:
                bayesian_score += bayesian_analysis[num]['posterior_probability']
        confidence += (bayesian_score / len(pattern)) * 10
        
        # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ç¢ºç‡è©•ä¾¡
        monte_carlo = all_analysis['monte_carlo_simulation']
        monte_score = 0
        for num in pattern:
            if num in monte_carlo['probability_distribution']:
                monte_score += monte_carlo['probability_distribution'][num]
        confidence += (monte_score / len(pattern)) * 8
        
        # æ™‚ç³»åˆ—ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ è©•ä¾¡
        time_series = all_analysis['enhanced_time_series']
        momentum_score = 0
        for num in pattern:
            if num in time_series['momentum_indicators']:
                momentum_score += time_series['momentum_indicators'][num]['momentum_strength']
        confidence += (momentum_score / len(pattern)) * 5
        
        return min(95.0, max(5.0, confidence))
    
    def generate_fusion_patterns(self, data):
        """å…¨æ©Ÿèƒ½çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        all_analysis = self.analyze_all_functions(data)
        patterns = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: çµ±è¨ˆçš„æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern1 = self.generate_statistical_optimization_pattern(all_analysis)
        confidence1 = self.calculate_unified_confidence(pattern1, all_analysis)
        patterns.append({
            'numbers': pattern1,
            'confidence': confidence1,
            'strategy': 'çµ±è¨ˆçš„æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern2 = self.generate_machine_learning_pattern(all_analysis)
        confidence2 = self.calculate_unified_confidence(pattern2, all_analysis)
        patterns.append({
            'numbers': pattern2,
            'confidence': confidence2,
            'strategy': 'æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ç¢ºç‡è«–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern3 = self.generate_probabilistic_pattern(all_analysis)
        confidence3 = self.calculate_unified_confidence(pattern3, all_analysis)
        patterns.append({
            'numbers': pattern3,
            'confidence': confidence3,
            'strategy': 'ç¢ºç‡è«–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: æ™‚ç³»åˆ—åˆ†æã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern4 = self.generate_time_series_pattern(all_analysis)
        confidence4 = self.calculate_unified_confidence(pattern4, all_analysis)
        patterns.append({
            'numbers': pattern4,
            'confidence': confidence4,
            'strategy': 'æ™‚ç³»åˆ—åˆ†æã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern5 = self.generate_pattern_recognition_pattern(all_analysis)
        confidence5 = self.calculate_unified_confidence(pattern5, all_analysis)
        patterns.append({
            'numbers': pattern5,
            'confidence': confidence5,
            'strategy': 'ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: çµ±åˆæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        pattern6 = self.generate_integrated_optimization_pattern(all_analysis, patterns[:5])
        confidence6 = self.calculate_unified_confidence(pattern6, all_analysis)
        patterns.append({
            'numbers': pattern6,
            'confidence': confidence6,
            'strategy': 'çµ±åˆæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨æ©Ÿèƒ½çµ±åˆï¼‰'
        })
        
        return patterns
    
    def generate_statistical_optimization_pattern(self, all_analysis):
        """çµ±è¨ˆçš„æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        candidates = []
        
        # çµ±è¨ˆçš„æ¤œå®šã§æœ‰æ„ãªæ•°å­—
        statistical_tests = all_analysis['statistical_tests']
        if 'chi_square_test' in statistical_tests:
            chi_sq = statistical_tests['chi_square_test']
            if chi_sq.get('p_value_estimate', 1) > 0.05:
                # ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒç¢ºèªã•ã‚ŒãŸå ´åˆã€ç†è«–å€¤ã«è¿‘ã„æ•°å­—ã‚’é¸æŠ
                theoretical = all_analysis['theoretical_analysis']['deviation_analysis']
                sorted_theoretical = sorted(theoretical.items(), key=lambda x: abs(x[1]['deviation']))
                candidates.extend([num for num, _ in sorted_theoretical[:20]])
        
        # ãƒ™ã‚¤ã‚ºç¢ºç‡ã®é«˜ã„æ•°å­—
        bayesian = all_analysis['bayesian_analysis']
        sorted_bayesian = sorted(bayesian.items(), key=lambda x: x[1]['posterior_probability'], reverse=True)
        candidates.extend([num for num, _ in sorted_bayesian[:15]])
        
        # é‡è¤‡é™¤å»ã—ã¦6å€‹é¸æŠ
        unique_candidates = list(dict.fromkeys(candidates))
        if len(unique_candidates) >= 6:
            return random.sample(unique_candidates, 6)
        else:
            # ä¸è¶³åˆ†ã‚’è£œå®Œ
            remaining = [i for i in range(1, 50) if i not in unique_candidates]
            return unique_candidates + random.sample(remaining, 6 - len(unique_candidates))
    
    def generate_machine_learning_pattern(self, all_analysis):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        candidates = []
        
        # å­¦ç¿’å±¥æ­´ã‹ã‚‰æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        learning = all_analysis['learning_analysis']
        if 'successful_numbers' in learning:
            sorted_learning = sorted(learning['successful_numbers'].items(), key=lambda x: x[1], reverse=True)
            candidates.extend([num for num, _ in sorted_learning[:10]])
        
        # é »åº¦åˆ†æã§å®‰å®šã—ãŸæ•°å­—
        frequency = all_analysis['frequency_analysis']
        sorted_frequency = sorted(frequency['frequency_distribution'].items(), key=lambda x: x[1], reverse=True)
        candidates.extend([num for num, _ in sorted_frequency[:15]])
        
        # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ã§ä¸Šæ˜‡ä¸­ã®æ•°å­—
        time_series = all_analysis['enhanced_time_series']
        positive_trend = [num for num, data in time_series['trend_analysis'].items() if data['trend'] > 0]
        candidates.extend(positive_trend[:10])
        
        # é‡è¤‡é™¤å»ã—ã¦6å€‹é¸æŠ
        unique_candidates = list(dict.fromkeys(candidates))
        if len(unique_candidates) >= 6:
            return random.sample(unique_candidates, 6)
        else:
            remaining = [i for i in range(1, 50) if i not in unique_candidates]
            return unique_candidates + random.sample(remaining, 6 - len(unique_candidates))
    
    def generate_probabilistic_pattern(self, all_analysis):
        """ç¢ºç‡è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        candidates = []
        
        # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®é«˜ç¢ºç‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        monte_carlo = all_analysis['monte_carlo_simulation']
        if 'top_patterns' in monte_carlo and monte_carlo['top_patterns']:
            top_pattern, _ = monte_carlo['top_patterns'][0]
            candidates.extend(list(top_pattern))
        
        # ãƒãƒ«ã‚³ãƒ•é€£é–ã®å®šå¸¸çŠ¶æ…‹ç¢ºç‡
        markov = all_analysis['markov_chain_analysis']
        if 'steady_state_probabilities' in markov:
            sorted_markov = sorted(markov['steady_state_probabilities'].items(), key=lambda x: x[1], reverse=True)
            candidates.extend([num for num, _ in sorted_markov[:12]])
        
        # ç¢ºç‡åˆ†å¸ƒã®é«˜ã„æ•°å­—
        probability_dist = monte_carlo['probability_distribution']
        sorted_prob = sorted(probability_dist.items(), key=lambda x: x[1], reverse=True)
        candidates.extend([num for num, _ in sorted_prob[:15]])
        
        # é‡è¤‡é™¤å»ã—ã¦6å€‹é¸æŠ
        unique_candidates = list(dict.fromkeys(candidates))
        if len(unique_candidates) >= 6:
            return random.sample(unique_candidates, 6)
        else:
            remaining = [i for i in range(1, 50) if i not in unique_candidates]
            return unique_candidates + random.sample(remaining, 6 - len(unique_candidates))
    
    def generate_time_series_pattern(self, all_analysis):
        """æ™‚ç³»åˆ—åˆ†æãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        candidates = []
        
        # æ™‚ç³»åˆ—ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®å¼·ã„æ•°å­—
        time_series = all_analysis['enhanced_time_series']
        momentum = time_series['momentum_indicators']
        positive_momentum = [num for num, data in momentum.items() if data['momentum_direction'] == 'positive']
        candidates.extend(positive_momentum[:10])
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã§ä¸Šæ˜‡ä¸­ã®æ•°å­—
        trend = time_series['trend_analysis']
        rising_trend = [num for num, data in trend.items() if data['trend'] > 0 and data['recent_frequency'] > data['overall_frequency']]
        candidates.extend(rising_trend[:10])
        
        # ãƒ•ãƒ¼ãƒªã‚¨åˆ†æã§å‘¨æœŸæ€§ã®å¼·ã„æ•°å­—
        fourier = all_analysis['fourier_analysis']
        if fourier:
            sorted_fourier = sorted(fourier.items(), key=lambda x: x[1]['periodicity_score'], reverse=True)
            candidates.extend([num for num, _ in sorted_fourier[:10]])
        
        # é‡è¤‡é™¤å»ã—ã¦6å€‹é¸æŠ
        unique_candidates = list(dict.fromkeys(candidates))
        if len(unique_candidates) >= 6:
            return random.sample(unique_candidates, 6)
        else:
            remaining = [i for i in range(1, 50) if i not in unique_candidates]
            return unique_candidates + random.sample(remaining, 6 - len(unique_candidates))
    
    def generate_pattern_recognition_pattern(self, all_analysis):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        candidates = []
        
        # é€£ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        consecutive = all_analysis['consecutive_analysis']
        if 'pairs' in consecutive and consecutive['pairs']:
            top_pairs = consecutive['pairs'].most_common(5)
            for pair, _ in top_pairs:
                candidates.extend(pair)
        
        # æ™‚é–“çš„ã‚µã‚¤ã‚¯ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        temporal = all_analysis['temporal_analysis']
        for weekday, numbers in temporal.items():
            if numbers:
                candidates.extend(numbers[:3])
        
        # ç¯„å›²ãƒãƒ©ãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        range_analysis = all_analysis['range_analysis']
        for range_type in ['low_freq', 'mid_freq', 'high_freq']:
            if range_type in range_analysis:
                top_range = range_analysis[range_type].most_common(3)
                candidates.extend([num for num, _ in top_range])
        
        # é‡è¤‡é™¤å»ã—ã¦6å€‹é¸æŠ
        unique_candidates = list(dict.fromkeys(candidates))
        if len(unique_candidates) >= 6:
            return random.sample(unique_candidates, 6)
        else:
            remaining = [i for i in range(1, 50) if i not in unique_candidates]
            return unique_candidates + random.sample(remaining, 6 - len(unique_candidates))
    
    def generate_integrated_optimization_pattern(self, all_analysis, previous_patterns):
        """çµ±åˆæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        # å‰ã®5ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æœ€é©ãªæ•°å­—ã‚’é¸æŠ
        all_numbers = []
        for pattern in previous_patterns:
            all_numbers.extend(pattern['numbers'])
        
        number_freq = Counter(all_numbers)
        top_numbers = [num for num, freq in number_freq.most_common(12)]
        
        # å…¨åˆ†ææ‰‹æ³•ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        weighted_scores = {}
        for num in top_numbers:
            score = number_freq[num] * 10  # åŸºæœ¬ã‚¹ã‚³ã‚¢
            
            # å„åˆ†ææ‰‹æ³•ã‹ã‚‰ã®ã‚¹ã‚³ã‚¢åŠ ç®—
            if num in all_analysis['bayesian_analysis']:
                score += all_analysis['bayesian_analysis'][num]['posterior_probability'] * 100
            
            if num in all_analysis['monte_carlo_simulation']['probability_distribution']:
                score += all_analysis['monte_carlo_simulation']['probability_distribution'][num] * 1000
            
            if num in all_analysis['enhanced_time_series']['momentum_indicators']:
                score += all_analysis['enhanced_time_series']['momentum_indicators'][num]['momentum_strength'] * 50
            
            weighted_scores[num] = score
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½6å€‹é¸æŠ
        sorted_numbers = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers[:6]]
    
    def predict_bonus(self, data, all_analysis):
        """ãƒœãƒ¼ãƒŠã‚¹æ•°å­—äºˆæ¸¬"""
        # æœ€è¿‘ã®ãƒœãƒ¼ãƒŠã‚¹æ•°å­—ã®å‚¾å‘
        recent_bonuses = [draw['additional'] for draw in data[-10:]]
        bonus_freq = Counter(recent_bonuses)
        
        # æœ€ã‚‚é »å‡ºã®ãƒœãƒ¼ãƒŠã‚¹æ•°å­—
        if bonus_freq:
            most_common_bonus = bonus_freq.most_common(1)[0][0]
            return most_common_bonus
        
        return random.randint(1, 49)
    
    def predict(self, target_date):
        """Ver.6 Ultimate Fusion äºˆæ¸¬å®Ÿè¡Œ"""
        print(f"ğŸš€ ToToã€‡ãã‚“ Ver.6 Ultimate Fusion - {target_date}äºˆæ¸¬")
        print("=" * 70)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = self.load_data()
        if not data:
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print(f"ğŸ¤– å…¨æ©Ÿèƒ½çµ±åˆåˆ†æå®Œäº†ï¼ˆ{len(data)}å›åˆ†ï¼‰")
        print(f"ğŸ§  AIé‡ã¿: {self.ai_weights}")
        
        # å…¨æ©Ÿèƒ½çµ±åˆåˆ†æ
        all_analysis = self.analyze_all_functions(data)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        patterns = self.generate_fusion_patterns(data)
        
        # ãƒœãƒ¼ãƒŠã‚¹äºˆæ¸¬
        bonus_prediction = self.predict_bonus(data, all_analysis)
        
        # åˆ†æçµæœè¡¨ç¤º
        print("ğŸ”¬ å…¨æ©Ÿèƒ½çµ±åˆåˆ†æçµæœ:")
        if 'statistical_tests' in all_analysis:
            stats = all_analysis['statistical_tests']
            if 'chi_square_test' in stats:
                chi_sq = stats['chi_square_test']
                print(f"   ğŸ“Š ã‚«ã‚¤äºŒä¹—æ¤œå®š: Ï‡Â²={chi_sq.get('chi_square_statistic', 0):.2f}, på€¤â‰ˆ{chi_sq.get('p_value_estimate', 0):.3f}")
        
        if 'monte_carlo_simulation' in all_analysis:
            monte = all_analysis['monte_carlo_simulation']
            if 'top_patterns' in monte and monte['top_patterns']:
                top_pattern, prob = monte['top_patterns'][0]
                print(f"   ğŸ¯ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ€é©ãƒ‘ã‚¿ãƒ¼ãƒ³: {list(top_pattern)} (ç¢ºç‡: {prob/1000:.3f})")
        
        if 'markov_chain_analysis' in all_analysis:
            markov = all_analysis['markov_chain_analysis']
            if 'steady_state_probabilities' in markov:
                steady_state = markov['steady_state_probabilities']
                top_steady = sorted(steady_state.items(), key=lambda x: x[1], reverse=True)[:3]
                print(f"   ğŸ”„ ãƒãƒ«ã‚³ãƒ•å®šå¸¸çŠ¶æ…‹ä¸Šä½: {[num for num, _ in top_steady]}")
        
        if 'enhanced_time_series' in all_analysis:
            time_series = all_analysis['enhanced_time_series']
            if 'momentum_indicators' in time_series:
                momentum_data = time_series['momentum_indicators']
                positive_momentum = [num for num, data in momentum_data.items() if data['momentum_direction'] == 'positive']
                if positive_momentum:
                    print(f"   ğŸ“ˆ æ™‚ç³»åˆ—ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¸Šä½: {positive_momentum[:5]}")
        
        print(f"ğŸ”¢ äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(patterns)}")
        print(f"ğŸ² ãƒœãƒ¼ãƒŠã‚¹äºˆæ¸¬: {bonus_prediction}")
        print()
        
        # çµæœå‡ºåŠ›
        for i, pattern in enumerate(patterns, 1):
            numbers = sorted(pattern['numbers'])
            confidence = pattern['confidence']
            strategy = pattern['strategy']
            total = sum(numbers)
            odd_count = len([n for n in numbers if n % 2 == 1])
            even_count = 6 - odd_count
            
            print(f"ã€ãƒ‘ã‚¿ãƒ¼ãƒ³{i}ã€‘ä¿¡é ¼åº¦: {confidence:.1f}% ({strategy})")
            print(f"äºˆæ¸¬æ•°å­—: {numbers}")
            print(f"åˆè¨ˆ: {total} | å¥‡æ•°/å¶æ•°: {odd_count}/{even_count}")
            print("-" * 70)
        
        print("ğŸ¯ Ver.6 Ultimate Fusion äºˆæ¸¬å®Œäº†ï¼")
        print("=" * 70)
        
        # çµæœä¿å­˜
        result_file = f"results/result_ver6_ultimate_fusion_{target_date}.txt"
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸš€ ToToã€‡ãã‚“ Ver.6 Ultimate Fusion - {target_date}äºˆæ¸¬\n")
            f.write("=" * 70 + "\n")
            f.write(f"ğŸ¤– å…¨æ©Ÿèƒ½çµ±åˆåˆ†æå®Œäº†ï¼ˆ{len(data)}å›åˆ†ï¼‰\n")
            f.write(f"ğŸ§  AIé‡ã¿: {self.ai_weights}\n")
            f.write("ğŸ”¬ å…¨æ©Ÿèƒ½çµ±åˆåˆ†æçµæœ:\n")
            
            if 'statistical_tests' in all_analysis:
                stats = all_analysis['statistical_tests']
                if 'chi_square_test' in stats:
                    chi_sq = stats['chi_square_test']
                    f.write(f"   ğŸ“Š ã‚«ã‚¤äºŒä¹—æ¤œå®š: Ï‡Â²={chi_sq.get('chi_square_statistic', 0):.2f}, på€¤â‰ˆ{chi_sq.get('p_value_estimate', 0):.3f}\n")
            
            if 'monte_carlo_simulation' in all_analysis:
                monte = all_analysis['monte_carlo_simulation']
                if 'top_patterns' in monte and monte['top_patterns']:
                    top_pattern, prob = monte['top_patterns'][0]
                    f.write(f"   ğŸ¯ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ€é©ãƒ‘ã‚¿ãƒ¼ãƒ³: {list(top_pattern)} (ç¢ºç‡: {prob/1000:.3f})\n")
            
            if 'markov_chain_analysis' in all_analysis:
                markov = all_analysis['markov_chain_analysis']
                if 'steady_state_probabilities' in markov:
                    steady_state = markov['steady_state_probabilities']
                    top_steady = sorted(steady_state.items(), key=lambda x: x[1], reverse=True)[:3]
                    f.write(f"   ğŸ”„ ãƒãƒ«ã‚³ãƒ•å®šå¸¸çŠ¶æ…‹ä¸Šä½: {[num for num, _ in top_steady]}\n")
            
            if 'enhanced_time_series' in all_analysis:
                time_series = all_analysis['enhanced_time_series']
                if 'momentum_indicators' in time_series:
                    momentum_data = time_series['momentum_indicators']
                    positive_momentum = [num for num, data in momentum_data.items() if data['momentum_direction'] == 'positive']
                    if positive_momentum:
                        f.write(f"   ğŸ“ˆ æ™‚ç³»åˆ—ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¸Šä½: {positive_momentum[:5]}\n")
            
            f.write(f"ğŸ”¢ äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(patterns)}\n")
            f.write(f"ğŸ² ãƒœãƒ¼ãƒŠã‚¹äºˆæ¸¬: {bonus_prediction}\n\n")
            
            for i, pattern in enumerate(patterns, 1):
                numbers = sorted(pattern['numbers'])
                confidence = pattern['confidence']
                strategy = pattern['strategy']
                total = sum(numbers)
                odd_count = len([n for n in numbers if n % 2 == 1])
                even_count = 6 - odd_count
                
                f.write(f"ã€ãƒ‘ã‚¿ãƒ¼ãƒ³{i}ã€‘ä¿¡é ¼åº¦: {confidence:.1f}% ({strategy})\n")
                f.write(f"äºˆæ¸¬æ•°å­—: {numbers}\n")
                f.write(f"åˆè¨ˆ: {total} | å¥‡æ•°/å¶æ•°: {odd_count}/{even_count}\n")
                f.write("-" * 70 + "\n")
            
            f.write("ğŸ¯ Ver.6 Ultimate Fusion äºˆæ¸¬å®Œäº†ï¼\n")
            f.write("=" * 70 + "\n")
        
        print(f"ğŸ’¾ çµæœã‚’ {result_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python predictor_ver6_ultimate_fusion.py YYYY-MM-DD")
        sys.exit(1)
    
    target_date = sys.argv[1]
    predictor = TotoVer6UltimateFusion()
    predictor.predict(target_date) 