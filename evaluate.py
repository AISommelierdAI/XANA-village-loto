#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import csv
from datetime import datetime
from typing import List, Dict, Tuple

class TotoEvaluator:
    def __init__(self, evaluation_file='evaluation_results.json'):
        """
        Totoäºˆæ¸¬è©•ä¾¡ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        """
        self.evaluation_file = evaluation_file
        self.results = self.load_evaluation_results()
        
    def load_evaluation_results(self) -> Dict:
        """
        è©•ä¾¡çµæœã‚’èª­ã¿è¾¼ã¿
        """
        try:
            with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
        except Exception as e:
            print(f"è©•ä¾¡çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def save_evaluation_results(self):
        """
        è©•ä¾¡çµæœã‚’ä¿å­˜
        """
        try:
            with open(self.evaluation_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è©•ä¾¡çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def evaluate_predictions(self, draw_date: str, predictions: List[Tuple[List[int], float]], 
                           actual_result: List[int]) -> Dict:
        """
        äºˆæ¸¬çµæœã‚’è©•ä¾¡
        
        Args:
            draw_date: æŠ½é¸æ—¥
            predictions: äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆ [(numbers, score), ...]
            actual_result: å®Ÿéš›ã®çµæœ
            
        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        actual_sorted = sorted(actual_result)
        evaluation = {
            'draw_date': draw_date,
            'actual_result': actual_sorted,
            'total_predictions': len(predictions),
            'predictions': [],
            'summary': {
                'best_hit_count': 0,
                'best_prediction_index': -1,
                'average_hit_count': 0.0,
                'hit_distribution': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
            }
        }
        
        total_hits = 0
        best_hits = 0
        best_index = -1
        
        for i, (numbers, score) in enumerate(predictions):
            sorted_numbers = sorted(numbers)
            hits = len(set(sorted_numbers) & set(actual_sorted))
            
            prediction_eval = {
                'index': i + 1,
                'predicted_numbers': sorted_numbers,
                'confidence_score': score,
                'hit_count': hits,
                'hit_numbers': list(set(sorted_numbers) & set(actual_sorted)),
                'missed_numbers': list(set(actual_sorted) - set(sorted_numbers)),
                'extra_numbers': list(set(sorted_numbers) - set(actual_sorted))
            }
            
            evaluation['predictions'].append(prediction_eval)
            evaluation['summary']['hit_distribution'][hits] += 1
            total_hits += hits
            
            if hits > best_hits:
                best_hits = hits
                best_index = i
        
        evaluation['summary']['best_hit_count'] = best_hits
        evaluation['summary']['best_prediction_index'] = best_index + 1
        evaluation['summary']['average_hit_count'] = total_hits / len(predictions)
        
        # çµæœã‚’ä¿å­˜
        self.results[draw_date] = evaluation
        self.save_evaluation_results()
        
        return evaluation
    
    def print_evaluation_summary(self, evaluation: Dict):
        """
        è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        """
        print(f"\nğŸ¯ {evaluation['draw_date']} è©•ä¾¡çµæœ")
        print("=" * 60)
        print(f"å®Ÿéš›ã®çµæœ: {evaluation['actual_result']}")
        print(f"äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {evaluation['total_predictions']}")
        print(f"æœ€é«˜ä¸€è‡´æ•°: {evaluation['summary']['best_hit_count']}/6")
        print(f"å¹³å‡ä¸€è‡´æ•°: {evaluation['summary']['average_hit_count']:.2f}/6")
        
        if evaluation['summary']['best_prediction_index'] > 0:
            best_pred = evaluation['predictions'][evaluation['summary']['best_prediction_index'] - 1]
            print(f"æœ€é«˜ä¸€è‡´äºˆæ¸¬: ãƒ‘ã‚¿ãƒ¼ãƒ³{best_pred['index']} - {best_pred['predicted_numbers']}")
            print(f"  ä¿¡é ¼åº¦: {best_pred['confidence_score']:.1f}%")
            print(f"  ä¸€è‡´æ•°å­—: {best_pred['hit_numbers']}")
        
        print("\nä¸€è‡´æ•°åˆ†å¸ƒ:")
        for hits, count in evaluation['summary']['hit_distribution'].items():
            if count > 0:
                print(f"  {hits}å€‹ä¸€è‡´: {count}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        
        print("=" * 60)
    
    def export_evaluation_csv(self, csv_file='evaluation_summary.csv'):
        """
        è©•ä¾¡çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        """
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'DrawDate', 'ActualResult', 'BestHitCount', 'BestPredictionIndex',
                    'AverageHitCount', 'TotalPredictions', 'HitDistribution'
                ])
                
                for draw_date, evaluation in self.results.items():
                    writer.writerow([
                        draw_date,
                        str(evaluation['actual_result']),
                        evaluation['summary']['best_hit_count'],
                        evaluation['summary']['best_prediction_index'],
                        f"{evaluation['summary']['average_hit_count']:.2f}",
                        evaluation['total_predictions'],
                        str(evaluation['summary']['hit_distribution'])
                    ])
            
            print(f"âœ… è©•ä¾¡çµæœã‚’ {csv_file} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_performance_trends(self) -> Dict:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ¨ç§»ã‚’åˆ†æ
        """
        if not self.results:
            return {}
        
        trends = {
            'total_draws': len(self.results),
            'average_best_hits': 0.0,
            'average_average_hits': 0.0,
            'hit_improvement': {},
            'best_performance': {'date': '', 'hits': 0},
            'worst_performance': {'date': '', 'hits': 6}
        }
        
        total_best_hits = 0
        total_average_hits = 0
        
        for draw_date, evaluation in self.results.items():
            best_hits = evaluation['summary']['best_hit_count']
            avg_hits = evaluation['summary']['average_hit_count']
            
            total_best_hits += best_hits
            total_average_hits += avg_hits
            
            # æœ€é«˜ãƒ»æœ€ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è¨˜éŒ²
            if best_hits > trends['best_performance']['hits']:
                trends['best_performance'] = {'date': draw_date, 'hits': best_hits}
            
            if best_hits < trends['worst_performance']['hits']:
                trends['worst_performance'] = {'date': draw_date, 'hits': best_hits}
        
        trends['average_best_hits'] = total_best_hits / len(self.results)
        trends['average_average_hits'] = total_average_hits / len(self.results)
        
        return trends
    
    def print_performance_trends(self):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»ã‚’è¡¨ç¤º
        """
        trends = self.get_performance_trends()
        if not trends:
            print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»")
        print("=" * 60)
        print(f"ç·è©•ä¾¡å›æ•°: {trends['total_draws']}å›")
        print(f"å¹³å‡æœ€é«˜ä¸€è‡´æ•°: {trends['average_best_hits']:.2f}/6")
        print(f"å¹³å‡ä¸€è‡´æ•°: {trends['average_average_hits']:.2f}/6")
        print(f"æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {trends['best_performance']['date']} ({trends['best_performance']['hits']}/6)")
        print(f"æœ€ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {trends['worst_performance']['date']} ({trends['worst_performance']['hits']}/6)")
        print("=" * 60) 